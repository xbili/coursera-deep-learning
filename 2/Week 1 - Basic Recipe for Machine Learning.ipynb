{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Dev/Test\n",
    "\n",
    "Back in GOML we always have our dataset split into:\n",
    "\n",
    "1. Training set\n",
    "2. Hold-out validation set (Dev)\n",
    "3. Test set\n",
    "\n",
    "In the first few lectures Andrew did mention that Deep Learning's performance increases with the amount of data used - unlike GOML algorithms which plateau after a certain time. The tried and tested way of 60/20/20 or other variants of splitting up our dataset would not make much sense for deep learning applications.\n",
    "\n",
    "In fact, we want as much training data as possible, and get **just enough** data for validation. If we have a dataset of 100 million, we wouldn't need 20 million examples for CV and testing. As such, a split such as 98/1/1 would make sense too.\n",
    "\n",
    "What is important about the test and dev set is that they come from the **same distribution**. E.g. if our test set are cat pictures found online, our dev set should also be of the same quality, rather than user uploaded cat pictures.\n",
    "\n",
    "In some cases, we do not need to have a **test set**, just a **dev set** for validation will do.\n",
    "\n",
    "## A Personal Note\n",
    "\n",
    "I've always liked to use `sklearn`'s `train_test_split` as a utility to split my dataset up, especially for Kaggle competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias/Variance Tradeoff\n",
    "\n",
    "This is yet another concept that was covered in CS229. Here's just a set of rules to follow for each case.\n",
    "\n",
    "## High Bias - Low Variance\n",
    "\n",
    "You are **UNDERFITTING**. \n",
    "\n",
    "Possible solutions:\n",
    "\n",
    "- Train for more iterations\n",
    "- Expand the size of network (number of hidden layers)\n",
    "- Try other neural network architectures\n",
    "\n",
    "## Low Bias - High Variance\n",
    "\n",
    "You are **OVERFITTING**.\n",
    "\n",
    "Possible solutions:\n",
    "\n",
    "- Get more data\n",
    "- Regularize (explained below)\n",
    "- Try other neural network architectuires\n",
    "\n",
    "---\n",
    "\n",
    "In GOML, we do not have the tools to optimize our model without avoiding a tradeoff. However in Deep Learning era, it is possible to reduce one without touching the other. Also, **creating a bigger network almost always works**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
