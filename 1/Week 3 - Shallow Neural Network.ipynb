{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations\n",
    "\n",
    "\\[1\\]: First layer\n",
    "\n",
    "\\[2\\]: Second layer\n",
    "\n",
    "...\n",
    "\n",
    "\\[n\\]: n-th layer\n",
    "\n",
    "From the computation graph below, we can deduce the associated notation of each layer of our neural network. This is what we will dive into for this week's videos. \n",
    "\n",
    "![Neural Network Computation Graph](images/nn-graph-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Representation\n",
    "\n",
    "We start with giving some names for easier reference of our neural network representation.\n",
    "\n",
    "1. Input layer: takes in the inputs directly\n",
    "2. Hidden layer: layers/nodes between input and output layers\n",
    "3. Output layer: the final activation before output $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network Representation](images/neural-network-representation-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative notation to represent the inputs to our neural network is to use $a^{[0]}$ - the activations in the zero-th layer.\n",
    "\n",
    "Subsequently, the hidden layer will output $a^{[1]}$ etc. The $i$ node will generate $a^{[1]}_i$.\n",
    "\n",
    "Finally, our $\\hat{y}$ can be denoted as $a^{[2]}$.\n",
    "\n",
    "We we see above is known as a **2 layer neural network**. By convention we do not count the input layer when naming neural networks.\n",
    "\n",
    "The parameters for each layer is denoted as $W^{[1]}$ and $b^{[1]}$ etc. with shape `(4, 3)` and `(4, 1)` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing a Neural Network's Output\n",
    "\n",
    "Recall that each node in the neural network computes $w^T x + b$ followed by an activation function $\\sigma$. Suppose that this node is now in hidden layer 1. The computation for the **first node** in hidden layer 1 would be\n",
    "\n",
    "$$z_1^{[1]} = w_1^{[1]T}x + b_1^{[1]}$$\n",
    "\n",
    "$$a_1^{[1]} = \\sigma(z_1^{[1]})$$\n",
    "\n",
    "Similarly, the second node would compute:\n",
    "\n",
    "$$z_2^{[1]} = w_2^{[1]T}x + b_2^{[1]}$$\n",
    "\n",
    "$$a_2^{[1]} = \\sigma(z_2^{[1]})$$\n",
    "\n",
    "The same is done for the 3rd and 4th nodes. We could do this in a neural network by running each neuron in the layer through a for loop. But as we learnt in previous videos, that is highly inefficient. So we try to **vectorize** this operation.\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "First we stack our 4 $w^{[1]T}$ vectors together to get a matrix of shape `(4, 3)`. Then we multiply this matrix by our $x$ and add the bias vector $b$ to it. Our end result will be a vector with each individual entry as:\n",
    "\n",
    "$$w_i^{[1]T}x + b_i^{[1]}$$\n",
    "\n",
    "![Vectorization](images/vectorize-nn-small.png)\n",
    "\n",
    "We can call this result as $z^{[1]}$. To find the output values for the layer, we just have to apply an activation function on each of the values in $z^{[1]}$. In this case it will be the sigmoid function. We call this result $a^{[1]}$.\n",
    "\n",
    "To calculate the output value $\\hat{y}$, we perform similar operations on the output layer as we did for the hidden layer by taking in $a^{[1]}$ as input for the last output layer neuron. In order to compute the final result, we perform the following:\n",
    "\n",
    "$$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$\n",
    "\n",
    "Then we apply the sigmoid function to obtain $a^{[2]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing across multiple examples\n",
    "\n",
    "The previous example of vectorization is only for one training example. What if we want to extend this to $m$ training examples? i.e. $x^{(1)}, x^{(2)}, ..., x^{(m)}$ to produce outputs $\\hat{y}^{(1)}, \\hat{y}^{(2)}, ..., \\hat{y}^{(m)}$.\n",
    "\n",
    "We introduce some new notations. $a^{[1](i)}$ will be the output from layer $1$ of training example $i$.\n",
    "\n",
    "Our algorithm will be as follow:\n",
    "\n",
    "![Vectorize Across Examples](./images/vectorize-across-examples-small.png)\n",
    "\n",
    "Recall that our training examples are stacked up **column-wise**, $X \\in \\mathbb{R}^{n_x \\times m}$.\n",
    "\n",
    "Therefore to compute our results in a vectorized manner,\n",
    "$$Z^{[1]} = W^{[1]}X + b^{[1]}$$\n",
    "$$A^{[1]} = \\sigma (Z^{[1]})$$\n",
    "$$Z^{[2]} = W^{[2]}X + b^{[2]}$$\n",
    "$$A^{[2]} = \\sigma (Z^{[2]})$$\n",
    "\n",
    "We can view $Z^{[1]}$, $Z^{[2]}$, as stacking $Z^{[1](1)}, Z^{[1](2)}, ..., Z^{[1](m)}$ **column-wise**. The same applies for $Z^{[2]}$, $A^{[1]}$ and $A^{[2]}$.\n",
    "\n",
    "Horizontally, we can view each column as a single training example. Vertically, we can see each row as a single node in the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justification of vectorized implementation\n",
    "\n",
    "Why does the previous two method of vectorization work? To gain some intuition and simplify our example, let's suppose that all our $b$ vectors are zero vectors.\n",
    "\n",
    "To calculate our $Z^{[1]}$ matrix, what we have is:\n",
    "$$Z^{[1](1)} = W^{[1]} x^{(1)}$$\n",
    "$$Z^{[1](2)} = W^{[1]} x^{(2)}$$\n",
    "$$Z^{[1](3)} = W^{[1]} x^{(3)}$$\n",
    "... and so on.\n",
    "\n",
    "We know that $W^{[1]} \\in \\mathbb{R}^{4 \\times n_x} $ (assuming 4 nodes in the hidden layer), and $x^{(i)} \\in \\mathbb{R}^{n_x \\times 1}$. Therefore each $Z^{[1](i)} \\in \\mathbb{R}^{4 \\times 1}$.\n",
    "\n",
    "By stacking them together **column-wise**, we get a matrix $Z^{[1]} \\in \\mathbb{R}^{4 \\times m}$.\n",
    "\n",
    "But instead of stacking them together **after** we multiply $W^{[1]}$ by each $x^{(i)}$, we can perform a matrix multiplication between $W^{[1]}$ and $X$, to get our $Z^{[1]}$ matrix in a single matrix multiply operation.\n",
    "\n",
    "This same logic can be used to justify for the other layers. Our algorithm now can be expressed as:\n",
    "\n",
    "![Multiple Examples Vectorized](./images/multiple-examples-vectorized-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code samples of the concept above\n",
    "\n",
    "In order to concretize what I've learnt, I will attempt to code out some examples of the above concepts to vectorize across training examples for forward propagation.\n",
    "\n",
    "Suppose that we have $m = 10$ inputs, each input has $n_x = 5$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n"
     ]
    }
   ],
   "source": [
    "m = 10\n",
    "n_x = 5\n",
    "\n",
    "# Our input matrix, 10 training examples and 5 features\n",
    "X = np.random.rand(n_x, m)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our first hidden layer in the neural network has $4$ neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "neurons_1 = 4\n",
    "\n",
    "W_1 = np.random.rand(neurons_1, n_x)\n",
    "print(W_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform the forward propagation computation **one training example at a time** in the explicit for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "[[ 1.05428913  0.91206421  0.72065878  0.75871072  0.99171047  1.25936009\n",
      "   0.84978966  1.32790531  0.80486291  1.30610292]\n",
      " [ 1.36851992  1.29523428  1.34127692  1.34803191  1.2420436   1.52586708\n",
      "   1.28892585  1.45701761  1.56523266  2.13654698]\n",
      " [ 1.24108459  1.14252411  1.33359829  1.2656931   0.98641853  1.42200879\n",
      "   1.16020329  1.19209331  1.45478729  2.00669077]\n",
      " [ 2.14739187  1.99111566  1.7567464   1.82524098  1.78567228  2.07381764\n",
      "   1.97287862  2.18875457  2.16366318  2.76867655]]\n"
     ]
    }
   ],
   "source": [
    "# Our z matrix should have the same number of rows as the number of neurons\n",
    "# and the same number of columns as training examples\n",
    "Z = np.zeros((neurons_1, m))\n",
    "\n",
    "for i in range(m):\n",
    "    # i-th training example\n",
    "    x_i = X[:,i].reshape(n_x, 1)\n",
    "    \n",
    "    # Good assertion practice as suggested by Andrew\n",
    "    assert(x_i.shape == (n_x, 1))\n",
    "    \n",
    "    Z[:,i] = np.dot(W_1, x_i).reshape(neurons_1)\n",
    "\n",
    "print(Z.shape)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But vectorize them and getting the exact same result in one single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "[[ 1.05428913  0.91206421  0.72065878  0.75871072  0.99171047  1.25936009\n",
      "   0.84978966  1.32790531  0.80486291  1.30610292]\n",
      " [ 1.36851992  1.29523428  1.34127692  1.34803191  1.2420436   1.52586708\n",
      "   1.28892585  1.45701761  1.56523266  2.13654698]\n",
      " [ 1.24108459  1.14252411  1.33359829  1.2656931   0.98641853  1.42200879\n",
      "   1.16020329  1.19209331  1.45478729  2.00669077]\n",
      " [ 2.14739187  1.99111566  1.7567464   1.82524098  1.78567228  2.07381764\n",
      "   1.97287862  2.18875457  2.16366318  2.76867655]]\n"
     ]
    }
   ],
   "source": [
    "Z_vectorized = np.dot(W_1, X)\n",
    "print(Z_vectorized.shape)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then applying the sigmoid activation function to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74159768  0.71342238  0.67275207  0.68107375  0.72942564  0.77891593\n",
      "   0.70052302  0.79049394  0.69101374  0.7868603 ]\n",
      " [ 0.79714092  0.78503183  0.79269985  0.79380768  0.77591953  0.82140081\n",
      "   0.78396532  0.8110761   0.82710292  0.89440493]\n",
      " [ 0.77575275  0.75814277  0.79143521  0.78000459  0.72837993  0.80565314\n",
      "   0.76136965  0.76711524  0.81073411  0.88149778]\n",
      " [ 0.89542481  0.87986112  0.8528017   0.86119382  0.85639587  0.88833223\n",
      "   0.87791997  0.89923511  0.89693866  0.94095951]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "A = sigmoid(Z)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above was an example of how these two operations are the same. The detailed justification is done by examining the matrix multiply operation carefully as Andrew explained in the video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
