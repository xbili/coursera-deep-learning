{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Neural Networks\n",
    "\n",
    "We are now ready to implement backpropagation for our neural network. Recall our formal definitions for a neural network.\n",
    "\n",
    "$n^{[0]}$ inputs, $n^{[1]}$ hidden units, $n^{[2]}$ output units.\n",
    "\n",
    "Parameters: \n",
    "$$w^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times n^{[0]}}$$\n",
    "$$b^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times 1}$$\n",
    "$$w^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times n^{[1]}}$$\n",
    "$$b^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times 1}$$\n",
    "\n",
    "Assuming we are doing **binary classification**, the cost function will be:\n",
    "$$J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}, y)$$\n",
    "\n",
    "After initializing the parameters, the algorithm goes as follow:\n",
    "1. Compute predictions $\\hat{y}$\n",
    "2. Compute derivatives, $\\delta W^{[1]}$, $\\delta b^{[1]}$ etc.\n",
    "3. Update $W^{[1]} = W^{[1]} - \\alpha \\delta W^{[1]}$ etc.\n",
    "\n",
    "And repeat these steps until the parameters look like they are converging.\n",
    "\n",
    "I would not write the formulas for the computations here since they can be easily referred to from the video again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional video appears to provide some cool insights on how these formulas are derived. Recall from the previous notebook how backpropagation for a single neuron logistic regression worked. Now instead of performing that for a single time, we perform that **twice**. Once for the hidden layer, and another time for the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our computation graph for our single layer neural network.\n",
    "\n",
    "![Neural Network Gradients](./images/neural-network-gradients-small.png)\n",
    "\n",
    "To calculate each of the partial derivatives in the backpropagation step, we can use the following formulas:\n",
    "\n",
    "![Backpropagation Derivatives](./images/backprop-derivatives-small.png)\n",
    "\n",
    "After vectorizing:\n",
    "\n",
    "![Backpropagation Vectorized](./images/backprop-vectorized-small.png)\n",
    "\n",
    "Note that the partial derivative of every variable is **equal** to the original variable. Therefore it is important to make sure that the dimensions of each matrix matches up during implementation. (Throw more asserts!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Initialization of Weights\n",
    "\n",
    "In logistic regression, we can initialize our weights to be 0 and start our gradient descent from there. However, this would not work in neural networks.\n",
    "\n",
    "Here's why, suppose that we initialize $W^{[1]}$ to a zero matrix. For any example that we give to the model, suppose $a_1^{[1]} = a_2^{[1]}$. Also, $dz_1^{[1]} = dz_2^{[1]}$. This means that two hidden units in a same layer are exactly the same. It is possible to show by induction that every unit in the hidden layer will be performing the same operation.\n",
    "\n",
    "Therefore we should always initialize $W^{[1]}$ and $W^{[2]}$ randomly, maybe multiply by a small $0.01$. $b^{[1]}$ and $b^{[2]}$ can be zero vectors. We want it to be a small number because of the way our activation functions are. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
