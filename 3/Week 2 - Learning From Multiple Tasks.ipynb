{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Suppose we trained a neural network for image recognition. If we want to transfer what we learned into another task, what we can do is to remove the last output layer of the neural network, and create a new output layer. Then we swap in a new dataset `(X, Y)`, initialize the last layer's weights randomly and retrain neural network on the new dataset.\n",
    "\n",
    "If we have limited data, we can just retrain the last output layer. If we have more data, we can retrain all of the parameters in the neural network.\n",
    "\n",
    "The initial phase of training before replacing the final layer is then called **pre-training**. The second phase of training is called **fine-tuning**.\n",
    "\n",
    "## Why does this work?\n",
    "\n",
    "Low level features can be learned from a very large dataset since it has already learned from the structure of a larger set of data.\n",
    "\n",
    "Transfer learning is the most effective if we have a lot of pre-training data, but relatively lesser data for our fine-tuning set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask Learning\n",
    "\n",
    "Start simultaneously with one neural network trying to do multiple task at the same time, in hope that each of these tasks will help the other tasks.\n",
    "\n",
    "An example is to have an autonomous driving car algorithm to recognize stop signs. Instead of just training a neural network to train on just identifying stop signs, we might want to let it train on more classes.\n",
    "\n",
    "So our output $\\hat{y}$ will be a multidimensional vector rather than a single label. Also, a single training example can have multiple labels instead of a single label. Our loss function used at the end is a normal sigmoid function and we sum up the four sigmoid values together.\n",
    "\n",
    "It is also possible for the labels to have no values in order to train the neural network. (The question marks) To train an algorithm like that, the loss function will only sum over loss values that aren't don't cares.\n",
    "\n",
    "The power of this lies in training a single neural network to perform better than training four separate neural networks.\n",
    "\n",
    "## When does this make sense?\n",
    "\n",
    "1. When the different problems share certain features\n",
    "2. Similar amount of data for each task\n",
    "3. Can train a big enougn neural network to do well on all the tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
