{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep L-layer Neural Network\n",
    "\n",
    "Here we introduce new notations for deep neural networks - neural networks with more than $1$ hidden layers.\n",
    "\n",
    "![Deep Neural Network](./images/deep-neural-network.png)\n",
    "\n",
    "We use:\n",
    "\n",
    "1. $L$ to denote the **number of layers** a deep neural network has - excluding the input layer\n",
    "2. $a^{[l]}$ denotes the **activation output** of the $l$-th layer\n",
    "3. $z^{[l]}$ represents our **linear activation** of the $l$-th layer\n",
    "\n",
    "Basically, all notation that we learnt before now can be in a hidden layer with index more than $1$.\n",
    "\n",
    "## Shape of parameters\n",
    "\n",
    "It is easy to get the shape of our $W$ matrices confused in deep neural networks. A general rule of the shape of each $W$ matrix is that $W^{[l]}$'s shape is $(n^{[l]}, n^{[l-1]})$. i.e. the number of rows reflect the number of hidden units in the **current layer**, and the number of columns reflect the number of hidden units in the **previous layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in Deep Neural Networks\n",
    "\n",
    "The formula for forward propagation is pretty straightforward.\n",
    "\n",
    "For each layer $l$, we calculate (after vectorization):\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "Note that there is no obvious way (or rather - it is impossible) to vectorize the operations across the different layers, so an explicit `for` loop has to be used in order to perform forward propagation in a deep neural network.\n",
    "\n",
    "Below we will explain something extra that we have to do for our forward propagation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation in Deep Neural Networks\n",
    "\n",
    "In my opinion, backward propagation is slightly more tricky for deep neural networks not because of the mathematical operations. The calculations are pretty straightforward, but we have to be careful with how we pass the data around. To me, Andrew's method of using several `cache` to pass the values backwards is very elegant in deep neural networks. It solves the problem effectively without much overhead.\n",
    "\n",
    "I will implement this neural network from scratch without the help from Jupyter notebooks again just to internalize this method of implementing neural networks.\n",
    "\n",
    "Here is the diagram used to explain backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Forward/Backward Propagation](./images/forward-backward-propagation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Step in Forward Propagation\n",
    "\n",
    "Forward propagation is represented in the diagram by the blue arrows from left to right. Each box represents a single computation step with a linear activation followed by the ReLU/sigmoid activation. \n",
    "\n",
    "Each box also has an arrow pointing downwards. This is what we call the `cache`. We will 'cache' the $z^{[L]}$ value in each step of forward propagation in order to use it for the backward propagation later.\n",
    "\n",
    "In each step of the backward propagation, we will make use of $da^{[l+1]}$ in the $l$-th layer to calculate our `dW` and `db` for that layer in order to update our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas for Derivatives\n",
    "\n",
    "The derived formulas (vectorized) for backpropagation are as follows:\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} \\times g^{[l]}{'} (Z^{[l]})$$\n",
    "$$dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1]T}$$\n",
    "$$db^{[l]} = \\frac{1}{m} \\text{np.sum($dZ^{[l]}$, axis=1, keep_dims=True)}$$\n",
    "$$dA^{[l]} = W^{[l]T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why does this work?\n",
    "\n",
    "![Deep Representation Intuition](./images/deep-representation-intuition.png)\n",
    "\n",
    "We can see a deep neural network as one that first learns simple features such as the lines for the edges of the human face. Then we compose these features together in the next layer to learn something more complex such as a part of the face etc. Then we can compose those complex features again in the next layer to learn something even more complex - a face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters vs Parameters\n",
    "\n",
    "To formalize the definitions, hyperparameters are values that **affects the actual values of the parameters**. These include:\n",
    "\n",
    "1. Learning rate, $\\alpha$\n",
    "2. Number of iterations\n",
    "3. Number of hidden layers, $L$\n",
    "4. Number of hidden units, $n^{[1]}, n^{[2]} \\dots$\n",
    "5. Choice of activation functions, `sigmoid`, `relu`, `tanh` etc.\n",
    "\n",
    "It is also important to note that since deep learning is a very **empirical process**, i.e. we can only know what works through trial and error, we need to constantly try out different hyperparameter values in order to evaluate the effectiveness of our model.\n",
    "\n",
    "The changes in data and environment of our application also affects the best hyperparameter values. These all plays a role in how we tune the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Personal Conclusion\n",
    "\n",
    "This chapter may seem short because Andrew explicitly say that he wanted us to gain more intuition from doing the programming assignment. Personally I felt that the assignment was a little bit too guided, thus I will implement some of the concepts **from scratch** again. The code will be uploaded to the repository when it is done. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
