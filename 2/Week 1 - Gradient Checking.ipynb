{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "\n",
    "How do we determine if we have a correct implementation of backpropagation? Are there *test cases* for our deep learning code?\n",
    "\n",
    "Luckily, calculus and numerical analysis comes to save the day.\n",
    "\n",
    "To approximate our expected derivative, we use a small $\\epsilon$ value to compute:\n",
    "\n",
    "$$\\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon)}{2 \\epsilon}$$\n",
    "\n",
    "Then we compare this with our actual derivative. However in neural networks, we have several different values of $\\theta$.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "To implement this for neural networks, we first have to concatenate all our weight matrices and bias vectors into a single long vector. Then for each $i$, we compute:\n",
    "\n",
    "$$d \\theta_\\text{approx} [i] = \\frac{J(\\theta_1, \\theta_2, \\dots, \\theta_i + \\epsilon, \\dots) - J(\\theta_1, \\theta_2, \\dots, \\theta_i - \\epsilon, \\dots)}{2 \\epsilon}$$\n",
    "\n",
    "Then we calculate the *Euclidean distance* between our approximated derivative and the actual derivative. Euclidean distance can be computed by the following:\n",
    "\n",
    "$$\\frac{\\Vert d \\theta_{\\text{approx}} - d \\theta \\Vert}{\\Vert d \\theta_{\\text{approx}} \\Vert + \\Vert d \\theta \\Vert}$$\n",
    "\n",
    "We see if the value above is $\\approx \\epsilon$.\n",
    "\n",
    "## Some Practical Tips\n",
    "\n",
    "Gradient checking is meant for testing purposes, as such it is highly inefficient. Avoid using it in **production**.\n",
    "\n",
    "This gradient check only tells us if there exist a bug in the backprop, but the process of identifying which component causes the error is still up to the developer to identify. Therefore it is still important to comply to good coding practices. \n",
    "\n",
    "Gradient checking **does not work with dropout**. Turn off the dropout layers before attempting this.\n",
    "\n",
    "It is also important to run gradient check after running a few iterations of training to verify correctness of backpropagation as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
