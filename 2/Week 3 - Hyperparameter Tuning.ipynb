{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Process\n",
    "\n",
    "In order of importance:\n",
    "\n",
    "1. $\\alpha$ - learning rate\n",
    "2. $\\beta$ - momentum term\n",
    "3. Mini-batch size\n",
    "4. Number of hidden units\n",
    "5. Number of layers\n",
    "6. Learning rate decay\n",
    "\n",
    "If we use Adam, we almost never tune $\\beta_1$, $\\beta_2$ and $\\epsilon$.\n",
    "\n",
    "## Hyperparameter Selection\n",
    "\n",
    "In the past, we can explore the values using a simple **grid search**. This is fine when the number of hyperparameters is relatively small.\n",
    "\n",
    "But for deep learning, it is more effective to choose the set of points using **random sampling**. Why? It is difficult to know in advance which hyperparameters are the most important. \n",
    "\n",
    "### Coarse-to-fine\n",
    "\n",
    "Once we find a region that produces good results, we can then proceed to focus on a smaller square for a denser search. This is known as **coarse to fine** search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an Appropriate Scale to select Hyperparameters\n",
    "\n",
    "Suppose we are trying to tune the number of hidden units $n^{[l]}$ for layer $l$. Suppose we want our value to be between $50$ to $100$. We can sample at random with a *uniform distribution*.\n",
    "\n",
    "## Learning Rate $\\alpha$\n",
    "\n",
    "This might not always be the case. Suppose now we want to tune our learning rate $\\alpha$, if we sample randomly with a uniform distribution between $0.00001$ and $1$, we will be using 90% of our resources to search between $0.00001$ and 10% of our resources to search between $0.00001$ and $0.1$.\n",
    "\n",
    "To solve this, we can tune the hyperparameter on a **log-scale** instead. We have more resources dedicated to search between $0.00001$ and $0.1$.\n",
    "\n",
    "We can implement this as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000410995433191\n"
     ]
    }
   ],
   "source": [
    "r = -4 * np.random.rand() # Value between [-4, 0]\n",
    "alpha = np.power(10, r)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for Exponentially Weighted Averages\n",
    "\n",
    "Use a **log-scale** for this as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00364190512984\n"
     ]
    }
   ],
   "source": [
    "r = -3 * np.random.rand()\n",
    "beta = np.power(10, r)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "Why is the log scale so important? Sometimes the hyperparameter becomes very sensitive at smaller values. A log-scale mitigates that effect by exploring more in a certain range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning in Practice\n",
    "\n",
    "The rise of deep learning has led to a transfer of knowledge between different communities. NLP problems might be effective for computer vision applications etc.\n",
    "\n",
    "**Hyperparameters can become stale!** We should re-tune our hyperparameters every once in a while, re-evaluate our models to ensure that we are still happy with how it is performing on new data.\n",
    "\n",
    "Below are a few strategies to tune the hyperparameters.\n",
    "\n",
    "## Babysitting One Model (Panda approach)\n",
    "\n",
    "Simply patiently wait and nudge the model parameters up and down over time. This is the only approach you can take if you don't have enough computation power. This is used pretty frequently because of the sheer size of data that most practioners have.\n",
    "\n",
    "Pandas like to have one child and make sure they grow up well.\n",
    "\n",
    "## Training many models in Parallel (Caviar approach)\n",
    "\n",
    "We can train several models in parallel with different hyperparameters. This should be used only if we have the computation power to train several models at a single time.\n",
    "\n",
    "Fishes lays a bunch of eggs and hope that one of them do well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
