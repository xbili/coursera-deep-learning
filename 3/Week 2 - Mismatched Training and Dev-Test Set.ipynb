{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing on Different Distributions\n",
    "\n",
    "Suppose that we train a model using cat pictures from the web to predict whether pictures uploaded by users on their mobile phones are cat pictures.\n",
    "\n",
    "Suppose that we have only 10,000 pictures from our user app, and around 200,000 pictures crawled from the internet. \n",
    "\n",
    "## Option 1\n",
    "\n",
    "First we put the two datasets together. Then we shuffle it and split them into train/dev/test set. This should be avoided because we are not evaluating our model against an accurate portrayal of the real world data that we will be receiving.\n",
    "\n",
    "## Option 2\n",
    "\n",
    "Use the 200,000 images from the web as the training set, and use the mobile app data for the dev/test sets. There are problems with training sets coming from different distributions, but we will worry about that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance with Mismatched Data\n",
    "\n",
    "Suppose that we have a cat classifier example, where human error is around 0% (near perfect).\n",
    "\n",
    "If we have a training error of 1% and dev error of 10%, we can say that our model has a high variance. However, if the training set and dev set comes from different distribution, we cannot come to the same conclusion. Because the dev set might be a more difficult dataset as compared to the training set. \n",
    "\n",
    "In order to remove the two effects, we should define a new set of data called a **training-dev set**. It should have the same distribution as the training set, but you don't train a neural network on this.\n",
    "\n",
    "What we do:\n",
    "\n",
    "1. Perform the normal train/dev/test split\n",
    "2. Randomly shuffle training set and take a piece of the training set as the training-dev set\n",
    "3. Train neural network with training data, without the training-dev set\n",
    "4. Evaluate on training-dev set AND dev set for error analysis\n",
    "\n",
    "Suppose that:\n",
    "- Error on training set is 1%\n",
    "- Error on training-dev set is 9%\n",
    "- Error on dev error is 10%\n",
    "\n",
    "This mean that the neural network is overfitting (variance problem) and is not generalizing well to training-dev set which comes from the same distribution as training data. \n",
    "\n",
    "Now suppose that:\n",
    "- Error on training set is 1%\n",
    "- Error on training-dev set is 1.5%\n",
    "- Error on dev error is 10%\n",
    "\n",
    "Now we don't have a variance problem, but rather a **data mismatch** problem. \n",
    "\n",
    "Again, suppose that:\n",
    "- Error on training set is 10%\n",
    "- Error on training-dev set is 11%\n",
    "- Error on dev error is 12%\n",
    "\n",
    "And assume that human level approximate is 0%. This performance is indicative of an avoidable bias problem. \n",
    "\n",
    "Finally, suppose that:\n",
    "- Error on training set is 10%\n",
    "- Error on training-dev set is 11%\n",
    "- Error on dev error is 20%\n",
    "\n",
    "In this case, the avoidable bias is still quite high, the variance is small, but there is data mismatch.\n",
    "\n",
    "## General Principles\n",
    "\n",
    "Key quantities to look out for:\n",
    "\n",
    "1. Human error\n",
    "2. Training set error\n",
    "3. Training-dev set error\n",
    "4. Dev set error\n",
    "\n",
    "An even more general formulation can be tabulated:\n",
    "\n",
    "![Data Mismatch](./images/data-mismatch.png)\n",
    "\n",
    "There aren't many ways to address data mismatch, but there are a few ways to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Address Data Mismatch?\n",
    "\n",
    "We can try to carry out **manual error analysis** and understand differences between training and dev/test sets. Find out the difference between the training and dev sets. \n",
    "\n",
    "Another way is to collect data that is more similar to dev/test set to include into the training set.\n",
    "\n",
    "## Artificial Data Synthesis\n",
    "\n",
    "The example used in the lecture is to create new artificial sound data by appending car noise to \"The quick brown fox jumps over the lazy dog\".\n",
    "\n",
    "Note of caution: suppose we have 10,000 hours of data that was recorded and just one hour of car noise. We could repeat the car noise 10,000 times in order to add it to the data. The audio will sound perfectly fine to the human ear, but there is a risk that the algorithm will overfit to one hour of the car noise.\n",
    "\n",
    "No matter what artificial data we have, there is always a chance that we may overfit to a very small subset of our entire dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
