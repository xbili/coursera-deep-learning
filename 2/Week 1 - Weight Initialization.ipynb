{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients\n",
    "\n",
    "Suppose we have a L layer neural network. We know that to get our prediction $\\hat{y}$, we perform the following computation:\n",
    "\n",
    "$$\\hat{y} = W^{[L]}W^{[L-1]}W^{[L-2]} \\dots W^{[2]} W^{[1]} X$$\n",
    "\n",
    "Now suppose that each of our $W$ has the same values and shape, the operation is simplified to:\n",
    "\n",
    "$$\\hat{y} = W^{L} X$$\n",
    "\n",
    "Why is this a problem? Let's illustrate this with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a simple weight matrix with $1.5$ at its diagonals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5  0. ]\n",
      " [ 0.   1.5]]\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have this simple weight matrix, W\n",
    "W = np.array([[1.5, 0], [0, 1.5]])\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we have a very deep neural network, with 100 layers. Observe that when we perform the computation for $\\hat{y}$, the weight values shoot up. This is known as **exploding gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.06561178e+17   0.00000000e+00]\n",
      " [  0.00000000e+00   4.06561178e+17]]\n"
     ]
    }
   ],
   "source": [
    "L = 100\n",
    "y = np.power(W, L)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, now suppose that our weight matrix are $0.5$ at its diagonals. Observe that the values are now very small. This is known as **vanishing gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.88860905e-31   0.00000000e+00]\n",
      " [  0.00000000e+00   7.88860905e-31]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[0.5, 0], [0, 0.5]])\n",
    "y = np.power(W, L)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to avoid this?\n",
    "\n",
    "To avoid this, we can use some clever intuition about the values of the weights to initialize. For a neuron with large number of inputs, the smaller we want the weight values to be.\n",
    "\n",
    "### Variance Scaling\n",
    "\n",
    "We can set the weight initialization to scale by setting the variance of our initial weight distribution to be \n",
    "$$\\frac{1}{n^{[L-1]}}$$\n",
    "or if we are using ReLU,\n",
    "$$\\frac{2}{n^{[L-1]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01114848  0.1224484   0.10872784]\n",
      " [-0.04284195  0.01937481 -0.02973807]\n",
      " [-0.04492172 -0.01220667  0.134523  ]\n",
      " [-0.03760996  0.03397368 -0.11037234]\n",
      " [ 0.06605986  0.09200254  0.11398231]]\n"
     ]
    }
   ],
   "source": [
    "inputs = 100\n",
    "shape = (5, 3)\n",
    "W = np.random.randn(shape[0], shape[1]) * np.sqrt(1 / inputs)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization\n",
    "\n",
    "We set variance to be:\n",
    "\n",
    "$$\\sqrt{\\frac{1}{n^{[L-1]}}}$$\n",
    "\n",
    "This is useful if we are using the `tanh` activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are bunch of other weight initialization strategies out there to be tried. On a personal note, Keras has a bunch of these `Initializer`s in their high level API for us to try out. Glad that they have those!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
