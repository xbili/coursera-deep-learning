{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's wrong with Batch Gradient Descent\n",
    "\n",
    "The kind of gradient descent that we learnt is known as **batch gradient descent**, where we group our entire training dataset as one huge matrix by stacking training examples column-wise and then performing forward propagation and backward propagation. \n",
    "\n",
    "However, if we have a large training set, it may take a very long time to compute our forward propagation and backward propagation step, therefore it will spend a long time trying to figure out the weight updates and the next step to take.\n",
    "\n",
    "If we were to perform the extreme opposite, i.e. train the neural network one training example at a time, we find that it takes very long as well since we are not exploiting vectorization. This is known as **stochastic gradient descent**. Also, it takes very long to converge.\n",
    "\n",
    "# Mini-batch Gradient Descent\n",
    "\n",
    "Mini-batch gradient descent is somewhere in the middle between **batch** and **stochastic** gradient descent. The idea is to split the training set into several 'baby training sets', and each 'baby' training set will be one step of the gradient descent algorithm.\n",
    "\n",
    "To indicate one entire pass of the training datset through the forward and back propagation step, we call it one single **epoch**.\n",
    "\n",
    "Suppose we have $m$ training examples with a mini-batch size of $32$. Each epoch will then have $\\frac{m}{32}$ steps.\n",
    "\n",
    "# Jagged Loss Graph\n",
    "\n",
    "If we plot the loss function $J$ as a function of the number of **steps** (note: not iterations), the graph will most likely have a jagged pattern. This is perfectly fine for mini-batch gradient descent, but definitely something wrong if we see this pattern for batch gradient descent.\n",
    "\n",
    "# Size of Mini-batch\n",
    "\n",
    "We can treat the size of the mini-batch as a hyperparameter to tune. But how?\n",
    "\n",
    "Some considerations are:\n",
    "\n",
    "1. Training set size: we can just use batch gradient descent if $m < 2000$.\n",
    "2. Typical batch size to choose is a power of 2 between $64$ to $512$ (power of 2 for hardware optimization)\n",
    "3. Make sure that each minibatch fits in the CPU/GPU!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
