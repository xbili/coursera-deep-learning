{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use dev set to evaluate ideas, and keep iterating using the dev set to improve performance of our model. Finally we test the performance of our model on the test set.\n",
    "\n",
    "# Distribution of Data\n",
    "\n",
    "Dev and test set should come from the **same distribution**.\n",
    "\n",
    "Here is an analogy, think of the dev set and our metric is the **bullseye target** that we want to shoot at. Iterating on this target will let us hit closer to the bullseye. If our test set is different from our dev set is akin to set a target at different distance. Then the months of work would be wasted.\n",
    "\n",
    "This is why we should avoid taking dev and test data from different distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of Dev/Test Sets\n",
    "\n",
    "## How to Split?\n",
    "\n",
    "The guidelines for the size of dev and test sets are changing in the deep learning era. In classic machine learning, we use a 70/30 split for train/dev split. Or a 60/20/20 for train/dev/test split.\n",
    "\n",
    "In modern machine learning era, we are used to working with much larger dataset sizes. Suppose that we have a **million** training example, we do not need to follow the previous guidelines. A 98/1/1 split might work well too.\n",
    "\n",
    "## Size of Test Set\n",
    "\n",
    "The test set needs to be big enough to give **high confidence** in the overall performance of the system. It would depend on how much data we have.\n",
    "\n",
    "## No-test Set\n",
    "\n",
    "Sometimes, we do not even need a test set. We only need a train and test set. In this case, we can just call the original 'test set' as a **dev set** instead.\n",
    "\n",
    "(In Kaggle, this is what I normally do. I use the leaderboard submission as a test set :P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Change Metrics or Train/Dev/Test Distribution?\n",
    "\n",
    "Suppose that we have two cat classification models, **A** and **B**. **A** has a lower error rate at 2%, while **B** has a higher error rate at 5%. However, **A** occasionally shows pornographic materials, therefore making this an inappropriate classifier. Misclassifying a cat is much less serious than showing pornographic materials to your users. \n",
    "\n",
    "In this case, we might want to change the metrics, or data distribution.\n",
    "\n",
    "We can change the error value to be calculated as:\n",
    "\n",
    "$$\\text{error} = \\frac{1}{m} \\sum_{i=0}^{m} W^{(i)} \\mathbb{I}\\{y_{\\text{pred}}^{(i)} \\neq y^{(i)}\\}$$\n",
    "\n",
    ", where:\n",
    "\n",
    "$$w_{\\text{porn}} = 10$$\n",
    "$$w_{\\text{not_porn}} = 1$$\n",
    "\n",
    "This way our error when an image is porn will be much higher. But this implies that we have to go through our dataset and label the pornographic images over again.\n",
    "\n",
    "The takeaway for this is if we are not satisfied with our old error metric, do not keep coasting along with it, but rather try and change the metric used in evaluating a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Orthogonalization\n",
    "\n",
    "The orthogonal conerns are as follow:\n",
    "\n",
    "1. Define a metric to evaluate classifiers (placing the target)\n",
    "2. Worry separately on how to do well on this metric (aiming and shooting at the target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
