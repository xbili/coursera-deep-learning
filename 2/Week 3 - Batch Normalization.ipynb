{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "The benefits of this technique:\n",
    "\n",
    "1. More robust neural network\n",
    "2. Increases the range of hyperparameters\n",
    "3. Easier to tune hyperparamters\n",
    "4. Easily train very deep networks\n",
    "\n",
    "## How it works?\n",
    "\n",
    "In batch gradient descent, normalizing our features can allow our neural network to train faster. It turns our loss space from something elongated to something more uniform.\n",
    "\n",
    "However for deeper models, at each layer the inputs may not be the normalized form. Won't it be nice if we can normalize them between each layer to help train each layer quickly as well?\n",
    "\n",
    "## Before or After Activation?\n",
    "\n",
    "In practice, normalizing **before** is done more frequently.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Given some intermediate values in the neural network, $z^{[1]} \\dots z^{[m]}$.\n",
    "\n",
    "$$\\mu = \\frac{1}{m} \\sum_{i} z^{(i(}$$\n",
    "$$\\sigma^2 = \\frac{1}{m} \\sum_{i} (z^{(i)} - \\mu)^2$$\n",
    "$$z^{(i)}_{\\text{norm}} = \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "$$\\tilde{z}^{(i)} = \\gamma z^{(i)}_{\\text{norm}} + \\beta$$\n",
    ",where $\\gamma$ and $\\beta$ are learnable parameters.\n",
    "\n",
    "Note that if:\n",
    "\n",
    "$$\\gamma = \\sqrt{\\sigma^2 + \\epsilon}$$\n",
    "$$\\beta = \\mu$$\n",
    "\n",
    ", then $\\tilde{z}^{(i)} = z^{(i)}$.\n",
    "\n",
    "Why should we have $\\gamma$ and $\\beta$? Let me craft out an example to convince myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Batch Norm into a Neural Network\n",
    "\n",
    "$\\beta^{[l]}$ and $\\gamma^{[l]}$ will now be added to the parameters to be trained. With these, we will also need $\\delta \\beta^{[l]}$ and $\\delta \\gamma^{[l]}$ during backpropagation.\n",
    "\n",
    "Fortunately, in most programming frameworks they provide us with the utilities for implementing batch normalization.\n",
    "\n",
    "## Working with Minibatches\n",
    "\n",
    "Normally rather than working with our entire training example, batch norm works best when used with a minibatch. \n",
    "\n",
    "Note that for batch normalization, we actually cancel out the effect that our bias vector has on $z$ values. This is because batch norm averages across the training examples, and adding a value to each of the training example will get cancelled out during this averaging step.\n",
    "\n",
    "Therefore we can leave our $b$ values as zero-vectors, or replace it with $\\beta$ parameter instead.\n",
    "\n",
    "## Implementing Gradient Descent with Batch Normalization\n",
    "\n",
    "In each minibatch $t$:\n",
    "\n",
    "1. Compute forward propagation on $X^{\\{t\\}}$\n",
    "2. In each hidden layer, replace $Z^{[l]}$ with ${\\tilde{Z^{[l]}}}$\n",
    "3. Use backprop to compute `dW`, `dbeta`, and `dgamma`\n",
    "4. Update parameters\n",
    "\n",
    "This also works with other optimization algorithms as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Intuition\n",
    "\n",
    "Why does batch normalization work? Let's explore a little bit on the intuition behind this trick.\n",
    "\n",
    "We saw how normalizing input features can speed up learning. This is now performing a similar effect, not just for input units, but for hidden units.\n",
    "\n",
    "## Covariate Shift\n",
    "\n",
    "Suppose we built a cat classifier that learns very well the images of black cats. But when we feed it pictures of white cats it may not perform very well. This effect is known as the **covariate shift**.\n",
    "\n",
    "The idea is that if we learn some $x$ and $y$ mapping, and if $x$'s distribution changes, then we have to retrain our entire model. Batch normalization ensures that the mean and variance of the $z$ value at each layer will stay at a certain mean and variance.\n",
    "\n",
    "It kind of allows each layer of the network to learn by itself, and **decouples** layers from each other.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Each minibatch is scaled by mean and variance of each minibatch. That mean and variance has a little bit of **noise** in it. It provides a slight regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization during Inference\n",
    "\n",
    "We come up with estimations of $\\mu$ and $\\sigma^2$ using exponentially weighted average (across mini-batches).\n",
    "\n",
    "Suppose we have minibatches $X^{\\{1\\}}, X^{\\{2\\}}, X^{\\{3\\}}, \\dots$, we calculate the exponentially weighted of the mean of each minibatch.\n",
    "\n",
    "Then we calculate the $z_{\\text{norm}}$ using our estimated $\\mu$ and $\\sigma^2$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
