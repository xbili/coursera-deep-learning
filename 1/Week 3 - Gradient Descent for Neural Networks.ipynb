{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Neural Networks\n",
    "\n",
    "We are now ready to implement backpropagation for our neural network. Recall our formal definitions for a neural network.\n",
    "\n",
    "$n^{[0]}$ inputs, $n^{[1]}$ hidden units, $n^{[2]}$ output units.\n",
    "\n",
    "Parameters: \n",
    "$$w^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times n^{[0]}}$$\n",
    "$$b^{[1]} \\in \\mathbb{R}^{n^{[1]} \\times 1}$$\n",
    "$$w^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times n^{[1]}}$$\n",
    "$$b^{[2]} \\in \\mathbb{R}^{n^{[2]} \\times 1}$$\n",
    "\n",
    "Assuming we are doing **binary classification**, the cost function will be:\n",
    "$$J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}, y)$$\n",
    "\n",
    "After initializing the parameters, the algorithm goes as follow:\n",
    "1. Compute predictions $\\hat{y}$\n",
    "2. Compute derivatives, $\\delta W^{[1]}$, $\\delta b^{[1]}$ etc.\n",
    "3. Update $W^{[1]} = W^{[1]} - \\alpha \\delta W^{[1]}$ etc.\n",
    "\n",
    "And repeat these steps until the parameters look like they are converging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
