{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "The process of manually inspecting the dataset that is miscategorized.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "1. Get ~100 mislabeled dev set examples\n",
    "2. Examine them manually\n",
    "3. Calculate how much error will be reduced if we were to focus on solving a particular misclassification problem\n",
    "\n",
    "We call the best case that working on a specific problem help our model as the **ceiling**.\n",
    "\n",
    "For example, if we find out that working on a certain problem only reduces the error by 1%, then it might not be worth the effort to spend time trying to fix it.\n",
    "\n",
    "## Evaluate Multiple Ideas in Parallel\n",
    "\n",
    "Suppose we find out that there are 3 different kinds of problem. We can come up with the following table, with one row for each **misclassified** image, to calculate how many % of the **total** images that has that checkmark. The outcome of this analysis will give us a sense of the best options to pursue.\n",
    "\n",
    "![Errors](./images/errors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up Incorrectly Labeled Data\n",
    "\n",
    "What happens when we have incorrectly labeled $y$ values? It turns out that deep learning algorithms are pretty **robust** to *random errors* in the training set. If these errors are random, the algorithm might be okay so long as the total dataset size is big enough.\n",
    "\n",
    "However, DL algorithsm are not as robust to *systematic errors*. If we consistently label an example incorrectly, then we cannot expect the model to work well. \n",
    "\n",
    "Incorrectly labeled examples can also exist in the dev set and test set. To find out if it is worth it to mitigate this, we can do the table above, and count the total % of incorrect examples that are misclassified. We only fix the incorrect labels if **it makes a significant difference**. It might not be the best use of your time if it does not affect the results that much.\n",
    "\n",
    "## Some tips\n",
    "\n",
    "- When cleaning up data, we need to apply the same process to **both dev and test sets** in order to ensure that they come from the same distribution.\n",
    "- It is also worth considering to examine examples that the algorithm got right\n",
    "- Train and dev/test data might now come from slightly different distributions (which is okay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Quick & Iterate\n",
    "\n",
    "Build the initial system quickly, and then iterate on that.\n",
    "\n",
    "1. Setup dev/test set and metric - this sets up the **target**.\n",
    "2. Build initial system quickly\n",
    "3. Use bias/variance analysis and Error Analysis\n",
    "\n",
    "The first system can be quick and dirty, but it is meant for us to figure out which of the many directions we can head towards. This advice applies less strongly if we have some significant prior experience for the application we are building."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
